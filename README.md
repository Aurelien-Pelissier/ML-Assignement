
# SampleHuman Assignment

**Self-Supervised Image Fragment Matching via Contrastive Representation Learning**

This project trains a neural encoder to produce meaningful embeddings for image **fragments**, such that fragments originating from the same image cluster together in embedding space. It uses self-supervised contrastive learning or pairwise binary cross-entropy (BCE) loss, and evaluates performance using metrics such as ARI, AUC, and MCC.

---

## ğŸ§ª Evaluation

### â¤ Run metrics on the validation set

By default, this samples **10 images at a time**, repeating the process 50Ã— to robustly evaluate the model on ~500 images (10% of the full validation set):

```bash
python collect_metrics.py --data_path "data" --model_checkpoint "model/best_model_CL_8.weights.h5"
````

This evaluates the model by:

* Reconstructing full images from fragments using **KMeans clustering**
* Measuring **Adjusted Rand Index (ARI)** to quantify clustering accuracy
* Reporting **MCC** (Matthews Correlation Coefficient) and **AUC** (ROC) using fragment similarity scores

---

### â¤ Visualize fragment embeddings

To visualize clustering and similarities for a **single batch of 10 validation images**, run:

```bash
python visualize_only.py --data_path "data" --model_checkpoint "model/best_model_CL_8.weights.h5"
```

This will generate:

* A 2D projection (UMAP or PCA) of fragment embeddings colored by image ID
* A similarity matrix (with optional clustering)
* Plots saved to the `Figure/` directory

---

## ğŸ” Training

To retrain the model from scratch using either contrastive loss (NT-Xent) or pairwise BCE:

```bash
# For contrastive loss
python train.py -CL

# For BCE loss
python train.py -BCE
```

Checkpoints will be saved to the `model/` folder as:

* `best_model_CL_<dim>.weights.h5`
* `best_model_BCE_<dim>.weights.h5`

You can change the embedding dimension or training steps by editing `train.py`.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ data/                 # ImageNet64 dataset
â”œâ”€â”€ model/                # Saved model weights
â”œâ”€â”€ Figure/               # Evaluation plots
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py          # Training and evaluation logic
â”‚   â”œâ”€â”€ collect_metrics.py # CLI evaluation script
â”‚   â”œâ”€â”€ visualize_only.py  # CLI visualization script
â”œâ”€â”€ README.md
```

---

## ğŸ“‹ Requirements

* Python 3.8+
* TensorFlow 2.x (with GPU support recommended)
* scikit-learn
* matplotlib
* umap-learn
* tqdm

Install all dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Method Overview

* Each 64Ã—64 image is split into 4Ã—4 = 16 fragments (16Ã—16 each)
* The encoder maps each fragment to a fixed-length embedding
* Training encourages fragments from the same image to be closer in embedding space

Two training losses available:

* `nt_xent_loss`: Normalized Temperature-scaled Cross Entropy (contrastive loss)
* `pairwise_bce_loss`: Binary classification on all fragment pairs (positive/negative)


